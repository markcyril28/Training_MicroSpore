# =============================================================================
# Micro Training Configuration - SERVER
# =============================================================================
# Optimized for: Dell Server with AMD MI210 + 128 CPU threads + 1TB RAM
#
# Usage:
#   python -m micro.ai.ml.trainer --config config/training_config_server.yaml
#
# Override any setting via command line:
#   python -m micro.ai.ml.trainer --config config/training_config_server.yaml --batch-size 512
# =============================================================================

# =============================================================================
# SERVER SPECS
# =============================================================================
#   GPU: AMD Instinct MI210 (Aldebaran/MI200)
#   VRAM: 64GB HBM2e
#   RAM: 1TB DDR4
#   Architecture: gfx90a (CDNA2)
#   Driver: amdgpu
#   Compute Platform: ROCm 6.x
#   CPU Threads: 128
#
# Thread Allocation (128 total):
#   - Self-play workers: 48 (~38%)
#   - DataLoader workers: 16 (~12%)
#   - OMP/MKL threads: 32 (~25%)
#   - System overhead: 32 (~25%)
# =============================================================================

# -----------------------------------------------------------------------------
# Device Settings
# -----------------------------------------------------------------------------
device:
  type: "cuda"                        # "cuda" (ROCm/HIP) for MI210
  amp:
    enabled: false                    # DISABLED: AMP may cause gradient instability on ROCm
                                      # Set to true to test; use bfloat16 if enabled
    dtype: "bfloat16"                 # "bfloat16" (recommended for MI210) or "float16"
  compile:
    enabled: true                     # torch.compile for kernel fusion
    mode: "reduce-overhead"           # "reduce-overhead" = slower compile, faster runtime
  matmul_precision: "medium"          # "medium" enables TF32-equivalent on supported hardware

# -----------------------------------------------------------------------------
# Self-Play Settings (Optimized for 128 CPU threads)
# -----------------------------------------------------------------------------
selfplay:
  cpu_workers: 32                     # ~38% of 128 threads for self-play
  games_per_epoch: 2048               # Balanced: enough data without long epoch times
  difficulties:                       # Cycle through for diverse training data
    - "easy"
    - "medium" 
    - "hard"
  focus_side: "both"                  # "white", "black", or "both"
  opponent_focus: "both"              # "ml", "algorithm", or "both"
  noise_prob: 0.20                    # 20% random moves for exploration
  max_moves_per_game: 100             # Limit game length for efficiency

# -----------------------------------------------------------------------------
# Training Hyperparameters (Optimized for 64GB HBM2e VRAM)
# -----------------------------------------------------------------------------
training:
  batch_size: 2048                    # Large batch for 64GB VRAM (FP32 mode)
                                      # Reduce to 1024 if OOM, increase to 4096 if stable
  learning_rate: 1.0e-4               # Lower LR for FP32 stability with large batch
  weight_decay: 1.0e-5                # L2 regularization
  grad_clip_norm: 1.0                 # Gradient clipping for stability (null to disable)
  train_steps: 100000000000           # Effectively unlimited
  checkpoint_every: 2000              # Save every 2000 steps

# -----------------------------------------------------------------------------
# DataLoader Settings (Optimized for 128 CPU threads + 1TB RAM)
# -----------------------------------------------------------------------------
dataloader:
  num_workers: 16                     # I/O bound - more than 16 rarely helps
  pin_memory: true                    # Faster GPU transfer with pinned memory
  prefetch_factor: 4                  # Prefetch 4 batches per worker for large batches
  
  # RAM Caching (1TB available - enables massive speedup)
  ram_cache:
    enabled: true                     # Pre-tensorize data in RAM (eliminates CPU bottleneck)
    threshold_gb: 64.0                # Require 64GB free to enable caching
    cache_file: null                  # Optional: "data/cache/dataset.pt" to persist

# -----------------------------------------------------------------------------
# Model Testing Settings (ML vs Algorithm)
# -----------------------------------------------------------------------------
testing:
  enabled: false                      # Disable for maximum training throughput
  every_n_steps: 5000                 # Test every 5000 steps when enabled
  num_games: 50                       # Games per evaluation
  difficulty: "medium"                # Algorithm difficulty for testing

# -----------------------------------------------------------------------------
# Paths
# -----------------------------------------------------------------------------
paths:
  checkpoint_dir: "models/checkpoints"
  latest_model: "models/latest.pt"
  replay_dir: "data/replay"
  log_dir: "logs"
  stats_file: "models/training_stats.json"

# -----------------------------------------------------------------------------
# Resume Settings  
# -----------------------------------------------------------------------------
resume:
  enabled: true                       # Auto-resume from latest valid checkpoint
  checkpoint_path: null               # null = auto-detect latest
  skip_corrupted: true                # Skip checkpoints with NaN/Inf

# -----------------------------------------------------------------------------
# Time-based Stopping
# -----------------------------------------------------------------------------
time_limit:
  enabled: true
  duration: "2h"                      # Training duration (e.g., "2h", "1d12h", "30m")
